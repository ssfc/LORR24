/*
TODO:
0) в PIBTS при подсчете скора не учитывается, как именно пойдут агенты этой операцией, возможно их операция вообще не согласована с GG
1) сделать более умный вес задачи у шедулера. Например: для каждой задачи найти самую ближайшую на ее конце и дать этому некоторый вес
    таким образом он будет не только брать самую близкую задачу, но еще и думать чуток наперед.
2) в PIBTS попробовать заложить в стоимость операции потенциальный конфликт при перестроении
3) Разобраться, почему в какой-то момент PIBTS очень долго работает. Выписать лог рекурсии, понять что он делает.
   Возможно к этому причастно какое-то удивительное состояние. Может быть есть роботы, которые близки к своему target.
   И они как-то плохо влияют на рекурсию. Пока я сам разбирался, то понял, что если ограничивать глубину рекурсии,
   то получается очень плохо и медленно. Так как он не находит результат и расширяется в дереве рекурсии,
   что ведет к росту сложности
4) Улучшить рекурсию в PIBTS. Как время так и качество результатов
5) Про GG: для каждой позиции на карте можно сделать число = -1, если не определено, 0-3 -- рекомендуемые направления движения.
   Если робот делает действие, которое не сходится с желаемым направлением (поворачивает в другую сторону, идет в другом направлении).
   То за это штраф некоторым числом, которое также подбирается в GraphGuidanceSolver. Если -1, то просто обычный вес.
   Если он идет в правильном направлении, то тоже обычный вес, который тоже подбирается в GGS.
   Далее эта штука когда нужно дать в lifelong, то GGS преобразует его в стандартную схему: weight[pos][dir][action].
   И подает на вход.
6) про PIBT:
вот мы хотим построить робота r. выбрали действие и видим там робота to_r, который нам мешает. Я сношу его, добавляю путь r и рекурсивно строю to_r. А почему бы не сделать так: рассмотреть пары (операция робота r, операция to_r). И конечно, чтобы они не коллизили друг с другом, но могут коллизить с другими роботами. И дать им приоритет = некоторой метрике об этих двух операциях. Отсортировать это и пройтись. Тут мы уже берем и ставим два пути, и возможно коллизим с третьим роботом, для него рекурсивно построить

ну или вообще строим для робота r. Переберем все его действия. если действие коллизит с другим роботом r2. То переберем еще и его действия. Но этот набор может коллизить с другим роботом r3. Которого мы уже рекурсивно построим. В итоге все это сортируем по весу, обходим в этом порядке и выполняем. Так мы даже сможем построить, если действие робота r коллизит с двумя роботами

Есть уже такое. Правда делалось это для оптимального алгоритма CBS, который не скейлится нифига)
Называется meta-agents.
Когда мы объединяем нескольких агентов в одну сущность и работаем с ним как с одним.

Таким образом мы можем решить вопрос с запуском рекурсии при конфликте с двумя агентами

Сейчас мы не понимаем на кого из них запустить рекурсию, а так мы запускаем её на одного мета-агента.

У такого подхода есть очевидное ограничение. Количество комбинаций действий растёт экспоненциально. Учитывая, что мы рассматриваем 16 действий, мета агент из 3х агентов - это уже 2048 возможных комбинаций.


UPD:
*)  имеет смысл также для random сделать так: сказать, что мы знаем когда конец и если агент не сможет выполнить задачу до этого момента, то у него нет задачи
    таким образом под конец шага мы уменьшим колво активных роботов, что должно улучшить скор
*)  WWW: мы должны были выяснить что сделать за эти WW. Мы должны перебрать CWW, RWW, RRW и взять лучший и сделать его
*)  (static_cast<int32_t>(robots.size()) - weight[r])
    может быть слишком большая разница между первым и последним
    может быть взять sqrt()
    может быть попробовать поделить на dist
    попробовать всякие разные веса
*)  "16 FFF FFW FWF FWW WFF WFW WWF FCF FRF RFF CFF RFW CFW RRF RWF CWF"
    попробовать добавить количество операций разной длины: F, FF, CF и прочее.
*/